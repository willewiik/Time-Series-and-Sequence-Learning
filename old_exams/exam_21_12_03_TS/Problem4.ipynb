{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4:\n",
    "\n",
    "(9p)\n",
    "\n",
    "**a)** When estimating the parameters of a model we are often focused on minimizing the squared loss function,\n",
    "$$\n",
    "    \\frac{1}{n}\\sum_{t=1}^{n} (y_t - \\hat{y}_{t|t-1}(\\theta))^2,\n",
    "$$\n",
    "where $\\hat{y}_t$ is our estimate (that depends on the parameters $\\theta$) of $y_t$.\n",
    "\n",
    "Assuming, as we have done during the course, that we have a model $y_t = h(y_{1:t-1}) + \\varepsilon_t$, where $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ and $h$ is some linear or non-linear function on the previous data, describe how the loss function realtes to the likelihood of the model.\n",
    "\n",
    "Would a change of distribution on $\\varepsilon_t$ change the related loss-function?\n",
    "\n",
    "<div style=\"text-align: right\"> (4p) </div>\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Together with your collaborator you face the problem of fitting an RNN model for an observed time series consisting of 50,000 observations, $\\{y_t\\}_{t=1}^{50000}$. For computational reasons, your collaborator has concluded that you can only afford to process 100 observations per gradient update when training the model. The following generator function has been written for this purpose, which will be passed to the Keras training procedure (`model.fit`).\n",
    "\n",
    "<div style=\"text-align: right\"> (1p) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train(window_size=100):    \n",
    "    # ntrain = number of training data points\n",
    "    while True:        \n",
    "        endPoint = np.random.randint(window_size, ntrain)\n",
    "        startPoint = endPoint - window_size\n",
    "        yield x_train[:,startPoint:endPoint,:], y_train[:,startPoint:endPoint,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Continuing from question **b)**. When studying the results you conclude that there are long ranging dependencies in the time series, much longer than your window size of 100. Explain a way to incorporate these long ranging dependencies in the training, note that you are still limited to 100 observations per gradient. Do you need to modify the generator from b)?\n",
    "\n",
    "Briefly explain the details of your proposed solution.\n",
    "<div style=\"text-align: right\"> (4p) </div>\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
